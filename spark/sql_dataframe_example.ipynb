{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame and SQL example\n",
    "\n",
    "Spark provides methods for efficiently processing structured data such as `.csv` files and unstructured data such as raw text files like the one we used for doing word counts.\n",
    "\n",
    "Code written using the DataFrame and SQL syntax in Python is just as efficient as code written in Scala/Java. This is a big difference when compared to Hadoop streaming where using Python code has performance impacts. In fact, DataFrame and SQL code expressed in Python are only \"wrappers\" around the Spark frame functions that have been implemented in Java. The actual processing is not exceuted in Python.\n",
    "\n",
    "This example is based on one found in: [Spark: The Definitive Guide](http://shop.oreilly.com/product/0636920034957.do).\n",
    "Download [this](https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/flight-data/csv/2015-summary.csv) file as example data. If the file can't be found anymore. We expect a `.csv` file with formated like:\n",
    "\n",
    "```\n",
    "DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count\n",
    "United States,Romania,15\n",
    "United States,Ireland,344\n",
    "Egypt,United States,15\n",
    "...\n",
    "```\n",
    "\n",
    "Summary:\n",
    "\n",
    "- Structured data should be processed with the DataFrame or SQL syntax. We will show both of these ways here.\n",
    "- Unstructured data should be procesed with the RDD syntax. We will show this in another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace your file location here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_location = \"./sample.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spark libraries\n",
    "Import the necessary Spark libraries. The entry point is always the `SparkSession` instance. If you run the `pyspark` shell then this session instance will already have been created for you. It's stored in the `spark` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the current Spark session or create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL/DataFrame basic example\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data. This creates a DataFrame. Replace `<your-file-location>` with the path where you stored the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark \\\n",
    "    .read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(sample_location)\n",
    "    \n",
    "# If you are really interested...\n",
    "# type(data)\n",
    "# dir(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the data using the DataFrame way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME=u'Egypt', ORIGIN_COUNTRY_NAME=u'United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'India', count=62)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Sort [count#376 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#376 ASC NULLS FIRST, 5)\n",
      "   +- *FileScan csv [DEST_COUNTRY_NAME#374,ORIGIN_COUNTRY_NAME#375,count#376] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/dkoch/Teaching/hadoop-training-exercises/spark/sample.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "data.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reference the column directly or by `data[\"count\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(max(data['count'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to return the Top-5 destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    "    .sum(\"count\") \\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\") \\\n",
    "    .sort(desc(\"destination_total\")) \\\n",
    "    .limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SQL way\n",
    "First create a view (table) representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"data_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first SQL query - simply return the maximum value of the `count` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) from data_view\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the same query as above - only expressed in SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|DEST_COUNTRY_NAME| total|\n",
      "+-----------------+------+\n",
      "|    United States|411352|\n",
      "|           Canada|  8399|\n",
      "|           Mexico|  7140|\n",
      "|   United Kingdom|  2025|\n",
      "|            Japan|  1548|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DEST_COUNTRY_NAME, sum(count) AS total\n",
    "    FROM data_view\n",
    "    GROUP BY\n",
    "        DEST_COUNTRY_NAME\n",
    "    ORDER BY total DESC\n",
    "    LIMIT 5\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
