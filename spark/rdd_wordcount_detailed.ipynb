{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A PySpark wordcount example\n",
    "\n",
    "This example aims to show in some more detail how MapReduce is used on Resiliant Distributed Datasets (RDD) in Spark to count words. You can add `.collect()` to the result of each transformation to see the intermediate output.\n",
    "\n",
    "See the Python [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html) on RDDs for more information. Note, that if your data has structure - something like .csv or regular JSON you should probably use [DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Spark libraries\n",
    "Import the necessary Spark libraries. The entry point is always the `SparkSession` instance. If you run the `pyspark` shell then this session instance will already have been created for you. It's stored in the `spark` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark wordcount\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in text. This creates an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.parallelize(['I love Hadoop,','but I do not love Hadopi.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first transformation replaces non-letters with spaces and lower cases text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_clean = lines.map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second transformation splits each line into words and then merges all words together.\n",
    "\n",
    "Input:\n",
    "\n",
    "```\n",
    "[\n",
    "    \"i love Hadoop\"\n",
    "    \"but i do not love Hadopi\"\n",
    "]\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "[\"i\", \"love\", \"hadoop\", \"but\", \"i\", \"do\", \"not\", \"love\", \"hadopi\"]\n",
    "```\n",
    "\n",
    "If we had used `map` instead of `flatMap` the output would have been:\n",
    "\n",
    "```\n",
    "[\n",
    "    [\"i\", \"love\", \"hadoop\"]\n",
    "    [\"but\", \"i\", \"do\", \"not\", \"love\", \"hadopi\"]\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_flat = lines_clean.flatMap(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third transformation emits a key-value pair `(word, 1)` for each word in the flat list. In Spark a key-value pair must be of type tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1),\n",
       " ('love', 1),\n",
       " ('hadoop', 1),\n",
       " ('but', 1),\n",
       " ('i', 1),\n",
       " ('do', 1),\n",
       " ('not', 1),\n",
       " ('love', 1),\n",
       " ('hadopi', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapWord(x):\n",
    "    return (x,1)\n",
    "\n",
    "words_mapped = words_flat.map(lambda x: (x, 1))\n",
    "words_mapped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth transformation groups all key-value pairs by key and sums the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 2),\n",
       " ('not', 1),\n",
       " ('love', 2),\n",
       " ('do', 1),\n",
       " ('hadoop', 1),\n",
       " ('hadopi', 1),\n",
       " ('but', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporary_result = words_mapped.reduceByKey(lambda x,y:x+y)\n",
    "temporary_result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth transformations simply revert the `(word, sum(word))` key-value pair so that the word becomes the value and the sum the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_result = temporary_result.map(lambda x:(x[1],x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final transformation sorts the keys (sum) in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = reversed_result.sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally claim the result by calling an action - retrieve the top 3 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'i'),\n",
       " (2, 'love'),\n",
       " (1, 'not'),\n",
       " (1, 'do'),\n",
       " (1, 'hadoop'),\n",
       " (1, 'hadopi'),\n",
       " (1, 'but')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_results.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do everything in one operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'i'),\n",
       " (2, 'love'),\n",
       " (1, 'not'),\n",
       " (1, 'do'),\n",
       " (1, 'hadoop'),\n",
       " (1, 'hadopi'),\n",
       " (1, 'but')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines \\\n",
    "    .map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(False) \\\n",
    "    .take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
